<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Manodeep Ray</title>
    <link>http://localhost:1313/website/projects/</link>
    <description>Recent content in Projects on Manodeep Ray</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Apr 2025 03:06:53 +0530</lastBuildDate>
    
        <atom:link href="http://localhost:1313/website/projects/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ClassIQ</title>
      <link>http://localhost:1313/website/projects/classiq/</link>
      <pubDate>Wed, 23 Apr 2025 03:06:53 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/classiq/</guid>
      <description>&lt;p&gt;Got it! Here&amp;rsquo;s the updated blog-style &lt;code&gt;.md&lt;/code&gt; with that additional context integrated. I&amp;rsquo;ve added a section near the top that sets the scene as a &lt;strong&gt;college minor project&lt;/strong&gt;, clarified the &lt;strong&gt;Streamlit demo&amp;rsquo;s purpose&lt;/strong&gt;, and explained how the &lt;strong&gt;server-based pipeline&lt;/strong&gt; works.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;building-a-classroom-monitoring-system-with-face-recognition-and-ai-powered-lecture-notes-&#34;&gt;Building a Classroom Monitoring System with Face Recognition and AI-Powered Lecture Notes üß†üì∏&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;üìö &lt;em&gt;This was my 3rd-year college minor project,&lt;/em&gt; built to explore how AI can enhance classroom engagement, automate attendance, and help with structured note generation.&lt;/p&gt;&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-vision&#34;&gt;üí° The Vision&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Classroom Monitoring System&lt;/strong&gt; is a server-based AI pipeline using &lt;strong&gt;computer vision&lt;/strong&gt;, &lt;strong&gt;speech recognition&lt;/strong&gt;, and &lt;strong&gt;LLMs&lt;/strong&gt; to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detect faces and recognize student identities&lt;/li&gt;
&lt;li&gt;Track attentiveness throughout a session&lt;/li&gt;
&lt;li&gt;Transcribe and summarize lectures into structured markdown notes&lt;/li&gt;
&lt;li&gt;Store and visualize data on a cloud dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the system is designed to work with &lt;strong&gt;edge devices&lt;/strong&gt; (like a classroom Raspberry Pi or local camera unit), I built a &lt;strong&gt;simple Streamlit app&lt;/strong&gt; to demo the functionality on a &lt;strong&gt;single video&lt;/strong&gt; on your laptop.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-it-works&#34;&gt;üß© How It Works&lt;/h2&gt;
&lt;p&gt;The core system is designed as a &lt;strong&gt;server-based pipeline&lt;/strong&gt;. Once deployed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;An &lt;strong&gt;edge device&lt;/strong&gt; records a classroom session (video + audio).&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;URL of the processing server (ngrok)&lt;/strong&gt; is entered into the edge device.&lt;/li&gt;
&lt;li&gt;Every time a new video is sent, the server:
&lt;ul&gt;
&lt;li&gt;Detects and recognizes faces&lt;/li&gt;
&lt;li&gt;Tracks attentiveness&lt;/li&gt;
&lt;li&gt;Transcribes the audio&lt;/li&gt;
&lt;li&gt;Uses an LLM to generate structured notes&lt;/li&gt;
&lt;li&gt;Saves all output to cloud storage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;Streamlit UI&lt;/strong&gt; (&lt;code&gt;apps/demo_app.py&lt;/code&gt;) showcases this pipeline for a &lt;strong&gt;single demo video&lt;/strong&gt;, giving a preview of the end-to-end process.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-features&#34;&gt;‚úÖ Core Features&lt;/h2&gt;
&lt;p&gt;‚úÖ &lt;strong&gt;Face Detection and Recognition for Attendance&lt;/strong&gt;&lt;br&gt;
‚úÖ &lt;strong&gt;Frame-Based Attentiveness Tracking&lt;/strong&gt;&lt;br&gt;
‚úÖ &lt;strong&gt;Audio Transcription via Vosk&lt;/strong&gt;&lt;br&gt;
‚úÖ &lt;strong&gt;LLM-Powered Summarization (Groq&amp;rsquo;s &lt;code&gt;llama3-8b-8192&lt;/code&gt;)&lt;/strong&gt;&lt;br&gt;
‚úÖ &lt;strong&gt;Streamlit-Based UI for Demos &amp;amp; Data Exploration&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tech-stack&#34;&gt;üß† Tech Stack&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Layer&lt;/th&gt;
          &lt;th&gt;Tools &amp;amp; Libraries&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Vision&lt;/td&gt;
          &lt;td&gt;YOLOv11, YuNet (OpenCV)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Transcription&lt;/td&gt;
          &lt;td&gt;&lt;a href=&#34;https://alphacephei.com/vosk/models&#34;&gt;Vosk Small EN&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Summarization&lt;/td&gt;
          &lt;td&gt;Groq LLM (&lt;code&gt;llama3-8b-8192&lt;/code&gt;)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Backend&lt;/td&gt;
          &lt;td&gt;Flask, tmux, ngrok&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;UI/Dashboard&lt;/td&gt;
          &lt;td&gt;Streamlit&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Deployment&lt;/td&gt;
          &lt;td&gt;Lightning AI&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-getting-started&#34;&gt;‚öôÔ∏è Getting Started&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Clone the repo&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone &amp;lt;repo-url&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd &amp;lt;repo-folder&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a virtual environment&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python3 -m venv venv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source venv/bin/activate  &lt;span style=&#34;color:#75715e&#34;&gt;# or venv\Scripts\activate on Windows&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install dependencies&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install -r requirements.txt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Install system packages (Linux)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install ffmpeg -y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# (Optional) Install tmux for background processes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install tmux
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Add your Groq API key to a .env file&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;GROQ_API_KEY=&amp;#34;your_key_here&amp;#34;&amp;#39;&lt;/span&gt; &amp;gt; .env
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## üöÄ Usage Instructions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;### üë§ Training Your Own Face Recognition Model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;1. Add your dataset: &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;dataset/&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;student_name&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;/&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;face_images&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2. Update the dataset path in &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;src/training/train_face_rec.py&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;3. Generate YOLO-ready data:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#e6db74&#34;&gt;```&lt;/span&gt;bash
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   python src/training/get_training_data.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Train the model:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bash scripts/train.sh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;-running-the-server-pipeline&#34;&gt;üñ•Ô∏è Running the Server Pipeline&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Optionally use tmux to manage long processes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bash scripts/run_servers.sh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Or run components manually&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python processor.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python server.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ngrok http &lt;span style=&#34;color:#ae81ff&#34;&gt;5000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;‚úÖ Once deployed, enter the &lt;strong&gt;ngrok link&lt;/strong&gt; in your edge device. Each uploaded classroom session video will be processed and results generated automatically.&lt;/p&gt;
&lt;h3 id=&#34;-try-the-demo-ui&#34;&gt;üåê Try the Demo UI&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Demo for single video&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;streamlit run apps/demo_app.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# View saved notes and data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;streamlit run apps/files_ui.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;-customizing-summarization&#34;&gt;üõ†Ô∏è Customizing Summarization&lt;/h2&gt;
&lt;p&gt;You can switch the LLM or update the prompt for note generation in:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# src/pipelines/core/llm_integration.py&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;GROQ_LLM_MODEL_ID &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;llama3-8b-8192&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And edit the prompt in &lt;code&gt;generate_notes()&lt;/code&gt; to match your desired note structure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-results--observations&#34;&gt;üìä Results &amp;amp; Observations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Face Recognition Accuracy&lt;/strong&gt;: 91.67% (Top-1)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attentiveness Metric&lt;/strong&gt;:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Attentiveness = (frames student is visible) / (total frames)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture Notes Quality&lt;/strong&gt;: Depends on LLM + prompt, human-evaluated&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-future-plans&#34;&gt;üîÆ Future Plans&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üß† Shift to custom facial-points-based recognition model&lt;/li&gt;
&lt;li&gt;üì¢ Add real-time inattentiveness alerts&lt;/li&gt;
&lt;li&gt;üß™ Refine LLM prompts for more structured results&lt;/li&gt;
&lt;li&gt;‚òÅÔ∏è Switch from CSV to PostgreSQL for scalable dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-about-me&#34;&gt;üôå About Me&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Manodeep Ray&lt;/strong&gt; ‚Äì Passionate about deep learning, LLM , CV, and building real-world systems. This project was built as part of my &lt;strong&gt;college 3rd-year minor&lt;/strong&gt;, blending CV + NLP in an educational setting.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-license&#34;&gt;üìÑ License&lt;/h2&gt;
&lt;p&gt;Licensed under &lt;strong&gt;Creative Commons (CC)&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thanks for checking it out! If you liked this, drop a ‚≠ê on &lt;a href=&#34;#&#34;&gt;GitHub&lt;/a&gt; or connect with me to chat about AI + education üöÄ&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
---

Let me know if you want a matching social media caption, GitHub repo description, or thumbnail image to go with this blog!
&lt;/code&gt;&lt;/pre&gt;</description>
      
      
    </item>
    
    <item>
      <title>AkamaiCare</title>
      <link>http://localhost:1313/website/projects/akamaicare/</link>
      <pubDate>Wed, 04 Sep 2024 22:41:27 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/akamaicare/</guid>
      <description>&lt;h2 id=&#34;akamaicare---ai-powered-healthcare-chatbot&#34;&gt;AkamaiCare - AI-Powered Healthcare Chatbot&lt;/h2&gt;
&lt;p&gt;AkamaiCare is an AI-driven chatbot designed to streamline hospital operations and enhance patient care through intelligent data querying and insights.&lt;/p&gt;
&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Natural Language Querying:&lt;/strong&gt; Allows hospital staff to retrieve information about patients, admissions, and claims processing using natural language input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Format Data Handling:&lt;/strong&gt; Processes data from CSV, PDF, and other document formats to ensure seamless integration with hospital records.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advanced Retrieval System:&lt;/strong&gt; Uses LlamaIndex to create QueryAgent and RetrieverAgent, enabling efficient parsing and retrieval of information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexible LLM Support:&lt;/strong&gt; Leverages Gemini Flash 1.5 API for NLP tasks, offering context-aware and accurate responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive Interface:&lt;/strong&gt; Developed using Streamlit for a responsive and user-friendly experience.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable Deployment:&lt;/strong&gt; Deployed on Hugging Face Spaces using Docker, ensuring high scalability and robust performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Retrieve details about patient admissions, discharge summaries, and billing records.&lt;/li&gt;
&lt;li&gt;Query claims processed by specific insurance providers over a given time period.&lt;/li&gt;
&lt;li&gt;Generate reports and insights to assist in hospital management and decision-making.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;github-repository&#34;&gt;GitHub Repository&lt;/h3&gt;
&lt;p&gt;Explore the complete code and implementation details on my &lt;a href=&#34;https://github.com/Manodeepray/akamai-care&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;live-demo&#34;&gt;Live Demo&lt;/h3&gt;
&lt;p&gt;Check out the live demo of AkamaiCare &lt;a href=&#34;https://akamai-care.streamlit.app/&#34;&gt;streamlit app&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for checking out AkamaiCare! Feel free to reach out with any feedback or questions.&lt;/p&gt;
</description>
      
      
    </item>
    
    <item>
      <title>Chat Bot for College</title>
      <link>http://localhost:1313/website/projects/chat-bot-for-college/</link>
      <pubDate>Wed, 04 Sep 2024 22:41:27 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/chat-bot-for-college/</guid>
      <description>&lt;h2 id=&#34;chat-bot-for-college&#34;&gt;Chat Bot for College&lt;/h2&gt;
&lt;p&gt;Welcome to my chatbot project for the School of Electronics, KIIT University! This chatbot is designed to make information retrieval easier and more efficient for students and faculty.&lt;/p&gt;
&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Built using a Retrieval-Augmented Generation (RAG) model for intelligent responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Processes data in .csv, .pdf, and .txt formats collected from the official SOEE, KIIT website.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Uses FAISS vector databases indexed with HuggingFace embeddings for efficient retrieval.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Employs an Ensemble Retriever with weighted averaging and a History-Aware Retriever for seamless multi-turn conversations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Supports Gemini 1.5 Flash, HuggingFace models (serverless inference), and Ollama on-device models, providing flexibility in model selection.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interactive and user-friendly interface created with Streamlit for smooth interactions and queries.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;github-repository&#34;&gt;GitHub Repository&lt;/h3&gt;
&lt;p&gt;Check out the code and details of this project on my &lt;a href=&#34;https://github.com/Manodeepray/kiit-chatbot-llm&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for exploring this project! Feel free to connect with me if you have suggestions or questions.&lt;/p&gt;
</description>
      
      
    </item>
    
    <item>
      <title>Oralcare_ai</title>
      <link>http://localhost:1313/website/projects/oralcare_ai/</link>
      <pubDate>Wed, 24 Jul 2024 12:32:52 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/oralcare_ai/</guid>
      <description>&lt;h2 id=&#34;its-my-summer-internship-project-at-iit-bhilai&#34;&gt;Its my Summer Internship Project at IIT Bhilai&lt;/h2&gt;
&lt;p&gt;During my internship, I worked on an innovative deep learning project focusing on cancer prediction. I developed a web app with Gradio that assists in diagnosing and evaluating oral diseases. Here‚Äôs a detailed breakdown of the project:&lt;/p&gt;
&lt;h3 id=&#34;part-1-image-classification&#34;&gt;Part 1: Image Classification&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; YOLOv8n&lt;br&gt;
&lt;strong&gt;Task:&lt;/strong&gt; Predicts the type of oral disease (cancer, dental caries, scurvy, periodontal, healthy) using image classification on custom data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced Detection:&lt;/strong&gt; If cancer is detected, the model then uses object detection to pinpoint the growth location and determines the grade and stage:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grades:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Grade 1:&lt;/strong&gt; Similar to normal cells&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grade 2:&lt;/strong&gt; Slightly abnormal&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grade 3:&lt;/strong&gt; Very abnormal&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Stages:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stage 1:&lt;/strong&gt; Less than 2 cm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stage 2:&lt;/strong&gt; 2 cm to 4 cm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stage 3:&lt;/strong&gt; Greater than 4 cm to widespread growth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data Collection:&lt;/strong&gt; All images were gathered and processed by me using Google extensions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Workflow:&lt;/strong&gt; The images are fed into the trained YOLOv8n model, which first classifies the disease type. If the disease is classified as cancer, the model performs object detection to identify the specific location of the cancerous growth. The detected area is then analyzed to determine the grade and stage of the cancer based on predefined criteria.&lt;/p&gt;
&lt;h3 id=&#34;part-2-symptom-based-prediction&#34;&gt;Part 2: Symptom-Based Prediction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; Symptoms entered by the patient&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Retrieval-Augmented Generation (RAG) model on custom PDFs&lt;br&gt;
&lt;strong&gt;Functionality:&lt;/strong&gt; Predicts the grade and stage of oral cancer based on patient symptoms, followed by treatment recommendations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grade and Stage Prediction:&lt;/strong&gt; The RAG model takes the patient‚Äôs symptoms and matches them with information from custom PDFs to determine the grade and stage of cancer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Treatment Recommendations:&lt;/strong&gt; Two additional RAG models provide treatment options based on the determined grade and stage, sourcing information from specific PDFs that detail treatments for each grade and stage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM used:&lt;/strong&gt; Mistral 7B&lt;/p&gt;
&lt;h3 id=&#34;challenges-and-limitations&#34;&gt;Challenges and Limitations&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;LLM Constraints:&lt;/strong&gt; Due to size limitations (maximum 10GB on Hugging Face), a lighter LLM was utilized, necessitating the creation of three distinct RAG models (two for treatment recommendations and one for grade and stage evaluation).&lt;br&gt;
&lt;strong&gt;Multiple API Calls:&lt;/strong&gt; Multiple calls to the LLM were required for coherent responses, which could be optimized with a more robust model such as GPT-3 or Gemini. A single, more powerful LLM would enhance processing efficiency and accuracy.&lt;br&gt;
&lt;strong&gt;Performance:&lt;/strong&gt; The current RAG model setup results in longer processing times, impacting the speed of output generation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/posts/manodeep-ray-346b3b294_deeplearning-cancerprediction-ai-activity-7219962641518067712--nht?utm_source=share&amp;amp;utm_medium=member_desktop&#34;&gt;Linked in post&lt;/a&gt;
&lt;a href=&#34;https://github.com/Manodeepray/oral_health_ai&#34;&gt;Github repo &lt;/a&gt;&lt;/p&gt;
</description>
      
      
    </item>
    
    <item>
      <title>Offline_ai</title>
      <link>http://localhost:1313/website/projects/offline_ai/</link>
      <pubDate>Wed, 24 Jul 2024 12:26:09 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/offline_ai/</guid>
      <description>&lt;h1 id=&#34;building-an-offline-sms-llm-app-for-buildspace-my-journey-and-challenges&#34;&gt;Building an Offline SMS LLM App for Buildspace: My Journey and Challenges&lt;/h1&gt;
&lt;p&gt;Hello everyone! It&amp;rsquo;s Manodeep here again, back with another update on my exciting adventures in the world of Machine Learning and AI. Today, I want to share my ongoing project for Buildspace and some of the challenges I&amp;rsquo;m facing along the way.&lt;/p&gt;
&lt;h2 id=&#34;the-project-offline-sms-llm-app&#34;&gt;The Project: Offline SMS LLM App&lt;/h2&gt;
&lt;p&gt;Currently, I&amp;rsquo;m working on an offline SMS LLM (Large Language Model) app for Buildspace. The idea is to create an application that leverages the power of language models to provide intelligent responses via SMS, all while working offline. This project is both challenging and exhilarating, pushing the boundaries of what can be achieved with limited connectivity.&lt;/p&gt;
&lt;h2 id=&#34;why-an-offline-sms-llm-app&#34;&gt;Why an Offline SMS LLM App?&lt;/h2&gt;
&lt;p&gt;In many parts of the world, internet access is still a luxury. By developing an offline SMS LLM app, I aim to bring the benefits of AI to areas with limited or no internet connectivity. This app can help with various tasks, from educational assistance to quick information retrieval, all through simple SMS.&lt;/p&gt;
&lt;h2 id=&#34;the-journey-so-far&#34;&gt;The Journey So Far&lt;/h2&gt;
&lt;h3 id=&#34;initial-research-and-planning&#34;&gt;Initial Research and Planning&lt;/h3&gt;
&lt;p&gt;The journey began with extensive research. I needed to understand the limitations and possibilities of running a language model offline and how to integrate it with SMS functionality. This involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Selecting the Right Model:&lt;/strong&gt; Choosing a language model that can operate efficiently offline. Smaller, more efficient models like Mistral 7B came into consideration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Understanding SMS Protocols:&lt;/strong&gt; Diving into the intricacies of SMS protocols and how to interact with them programmatically.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;development-phase&#34;&gt;Development Phase&lt;/h3&gt;
&lt;p&gt;With a solid plan in place, I started the development phase. Here&amp;rsquo;s a snapshot of the process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model Optimization:&lt;/strong&gt; Ensuring the language model can run smoothly on devices with limited resources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SMS Integration:&lt;/strong&gt; Developing the SMS interface to send and receive messages seamlessly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Interface:&lt;/strong&gt; Creating a simple and intuitive interface for users to interact with the app.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-progress&#34;&gt;The Progress&lt;/h2&gt;
&lt;h1 id=&#34;progess-till-24724&#34;&gt;Progess till 24.7.24&lt;/h1&gt;
&lt;p&gt;Now currently facing an issue of integrating the SMS interface . i have done everything but the number used by me is international number ..and my phone doesnt have international roaming :..so i cant test my app &amp;hellip; so yeaaaa.&lt;/p&gt;
</description>
      
      
    </item>
    
  </channel>
</rss>
