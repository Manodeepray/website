<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Manodeep Ray</title>
    <link>http://localhost:1313/website/projects/</link>
    <description>Recent content in Projects on Manodeep Ray</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Apr 2025 03:06:53 +0530</lastBuildDate>
    
        <atom:link href="http://localhost:1313/website/projects/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ClassIQ</title>
      <link>http://localhost:1313/website/projects/classiq/</link>
      <pubDate>Wed, 23 Apr 2025 03:06:53 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/classiq/</guid>
      <description>&lt;p&gt;Got it! Here&amp;rsquo;s the updated blog-style &lt;code&gt;.md&lt;/code&gt; with that additional context integrated. I&amp;rsquo;ve added a section near the top that sets the scene as a &lt;strong&gt;college minor project&lt;/strong&gt;, clarified the &lt;strong&gt;Streamlit demo&amp;rsquo;s purpose&lt;/strong&gt;, and explained how the &lt;strong&gt;server-based pipeline&lt;/strong&gt; works.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# Building a Classroom Monitoring System with Face Recognition and AI-Powered Lecture Notes üß†üì∏
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;&amp;gt; &lt;/span&gt;&lt;span style=&#34;font-style:italic&#34;&gt;üìö *This was my 3rd-year college minor project,* built to explore how AI can enhance classroom engagement, automate attendance, and help with structured note generation.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;font-style:italic&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## üí° The Vision
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The &lt;span style=&#34;font-weight:bold&#34;&gt;**Classroom Monitoring System**&lt;/span&gt; is a server-based AI pipeline using &lt;span style=&#34;font-weight:bold&#34;&gt;**computer vision**&lt;/span&gt;, &lt;span style=&#34;font-weight:bold&#34;&gt;**speech recognition**&lt;/span&gt;, and &lt;span style=&#34;font-weight:bold&#34;&gt;**LLMs**&lt;/span&gt; to:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Detect faces and recognize student identities
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Track attentiveness throughout a session
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Transcribe and summarize lectures into structured markdown notes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Store and visualize data on a cloud dashboard
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;While the system is designed to work with &lt;span style=&#34;font-weight:bold&#34;&gt;**edge devices**&lt;/span&gt; (like a classroom Raspberry Pi or local camera unit), I built a &lt;span style=&#34;font-weight:bold&#34;&gt;**simple Streamlit app**&lt;/span&gt; to demo the functionality on a &lt;span style=&#34;font-weight:bold&#34;&gt;**single video**&lt;/span&gt; on your laptop.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## üß© How It Works
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The core system is designed as a &lt;span style=&#34;font-weight:bold&#34;&gt;**server-based pipeline**&lt;/span&gt;. Once deployed:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;1.&lt;/span&gt; An &lt;span style=&#34;font-weight:bold&#34;&gt;**edge device**&lt;/span&gt; records a classroom session (video + audio).
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;2.&lt;/span&gt; The &lt;span style=&#34;font-weight:bold&#34;&gt;**URL of the processing server (ngrok)**&lt;/span&gt; is entered into the edge device.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;3.&lt;/span&gt; Every time a new video is sent, the server:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Detects and recognizes faces
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Tracks attentiveness
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Transcribes the audio
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Uses an LLM to generate structured notes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;-&lt;/span&gt; Saves all output to cloud storage
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The &lt;span style=&#34;font-weight:bold&#34;&gt;**Streamlit UI**&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;`apps/demo_app.py`&lt;/span&gt;) showcases this pipeline for a &lt;span style=&#34;font-weight:bold&#34;&gt;**single demo video**&lt;/span&gt;, giving a preview of the end-to-end process.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## ‚úÖ Core Features
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚úÖ &lt;span style=&#34;font-weight:bold&#34;&gt;**Face Detection and Recognition for Attendance**&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚úÖ &lt;span style=&#34;font-weight:bold&#34;&gt;**Frame-Based Attentiveness Tracking**&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚úÖ &lt;span style=&#34;font-weight:bold&#34;&gt;**Audio Transcription via Vosk**&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚úÖ &lt;span style=&#34;font-weight:bold&#34;&gt;**LLM-Powered Summarization (Groq&amp;#39;s `llama3-8b-8192`)**&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚úÖ &lt;span style=&#34;font-weight:bold&#34;&gt;**Streamlit-Based UI for Demos &amp;amp; Data Exploration**&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## üß† Tech Stack
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Layer             | Tools &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;amp;&lt;/span&gt; Libraries                            |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;|------------------|----------------------------------------------|
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Vision            | YOLOv11, YuNet (OpenCV)                      |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Transcription     | [&lt;span style=&#34;color:#f92672&#34;&gt;Vosk Small EN&lt;/span&gt;](&lt;span style=&#34;color:#a6e22e&#34;&gt;https://alphacephei.com/vosk/models&lt;/span&gt;) |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Summarization     | Groq LLM (&lt;span style=&#34;color:#e6db74&#34;&gt;`llama3-8b-8192`&lt;/span&gt;)                  |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Backend           | Flask, tmux, ngrok                           |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| UI/Dashboard      | Streamlit                                    |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| Deployment        | Lightning AI                                 |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## ‚öôÔ∏è Getting Started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;```bash
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# Clone the repo
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;repo-url&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;repo-folder&lt;/span&gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# Create a virtual environment
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python3 -m venv venv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source venv/bin/activate  # or venv\Scripts\activate on Windows
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# Install dependencies
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install -r requirements.txt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# Install system packages (Linux)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install ffmpeg -y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# (Optional) Install tmux for background processes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install tmux
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# Add your Groq API key to a .env file
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;echo &amp;#39;GROQ_API_KEY=&amp;#34;your_key_here&amp;#34;&amp;#39; &amp;gt; .env
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;-usage-instructions&#34;&gt;üöÄ Usage Instructions&lt;/h2&gt;
&lt;h3 id=&#34;-training-your-own-face-recognition-model&#34;&gt;üë§ Training Your Own Face Recognition Model&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Add your dataset: &lt;code&gt;dataset/{student_name}/{face_images}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update the dataset path in &lt;code&gt;src/training/train_face_rec.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Generate YOLO-ready data:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python src/training/get_training_data.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Train the model:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bash scripts/train.sh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;-running-the-server-pipeline&#34;&gt;üñ•Ô∏è Running the Server Pipeline&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Optionally use tmux to manage long processes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bash scripts/run_servers.sh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Or run components manually&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python processor.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python server.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ngrok http &lt;span style=&#34;color:#ae81ff&#34;&gt;5000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;‚úÖ Once deployed, enter the &lt;strong&gt;ngrok link&lt;/strong&gt; in your edge device. Each uploaded classroom session video will be processed and results generated automatically.&lt;/p&gt;
&lt;h3 id=&#34;-try-the-demo-ui&#34;&gt;üåê Try the Demo UI&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Demo for single video&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;streamlit run apps/demo_app.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# View saved notes and data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;streamlit run apps/files_ui.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;-customizing-summarization&#34;&gt;üõ†Ô∏è Customizing Summarization&lt;/h2&gt;
&lt;p&gt;You can switch the LLM or update the prompt for note generation in:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# src/pipelines/core/llm_integration.py&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;GROQ_LLM_MODEL_ID &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;llama3-8b-8192&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And edit the prompt in &lt;code&gt;generate_notes()&lt;/code&gt; to match your desired note structure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-results--observations&#34;&gt;üìä Results &amp;amp; Observations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Face Recognition Accuracy&lt;/strong&gt;: 91.67% (Top-1)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attentiveness Metric&lt;/strong&gt;:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Attentiveness = (frames student is visible) / (total frames)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture Notes Quality&lt;/strong&gt;: Depends on LLM + prompt, human-evaluated&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-future-plans&#34;&gt;üîÆ Future Plans&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üß† Shift to custom facial-points-based recognition model&lt;/li&gt;
&lt;li&gt;üì¢ Add real-time inattentiveness alerts&lt;/li&gt;
&lt;li&gt;üß™ Refine LLM prompts for more structured results&lt;/li&gt;
&lt;li&gt;‚òÅÔ∏è Switch from CSV to PostgreSQL for scalable dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-about-me&#34;&gt;üôå About Me&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Manodeep Ray&lt;/strong&gt; ‚Äì Passionate about deep learning, LLM , CV, and building real-world systems. This project was built as part of my &lt;strong&gt;college 3rd-year minor&lt;/strong&gt;, blending CV + NLP in an educational setting.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-license&#34;&gt;üìÑ License&lt;/h2&gt;
&lt;p&gt;Licensed under &lt;strong&gt;Creative Commons (CC)&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thanks for checking it out! If you liked this, drop a ‚≠ê on &lt;a href=&#34;#&#34;&gt;GitHub&lt;/a&gt; or connect with me to chat about AI + education üöÄ&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
---

Let me know if you want a matching social media caption, GitHub repo description, or thumbnail image to go with this blog!
&lt;/code&gt;&lt;/pre&gt;</description>
      
      
    </item>
    
    <item>
      <title>AkamaiCare</title>
      <link>http://localhost:1313/website/projects/akamaicare/</link>
      <pubDate>Wed, 04 Sep 2024 22:41:27 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/akamaicare/</guid>
      <description>&lt;h2 id=&#34;akamaicare---ai-powered-healthcare-chatbot&#34;&gt;AkamaiCare - AI-Powered Healthcare Chatbot&lt;/h2&gt;
&lt;p&gt;AkamaiCare is an AI-driven chatbot designed to streamline hospital operations and enhance patient care through intelligent data querying and insights.&lt;/p&gt;
&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Natural Language Querying:&lt;/strong&gt; Allows hospital staff to retrieve information about patients, admissions, and claims processing using natural language input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Format Data Handling:&lt;/strong&gt; Processes data from CSV, PDF, and other document formats to ensure seamless integration with hospital records.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Advanced Retrieval System:&lt;/strong&gt; Uses LlamaIndex to create QueryAgent and RetrieverAgent, enabling efficient parsing and retrieval of information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flexible LLM Support:&lt;/strong&gt; Leverages Gemini Flash 1.5 API for NLP tasks, offering context-aware and accurate responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive Interface:&lt;/strong&gt; Developed using Streamlit for a responsive and user-friendly experience.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable Deployment:&lt;/strong&gt; Deployed on Hugging Face Spaces using Docker, ensuring high scalability and robust performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;use-cases&#34;&gt;Use Cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Retrieve details about patient admissions, discharge summaries, and billing records.&lt;/li&gt;
&lt;li&gt;Query claims processed by specific insurance providers over a given time period.&lt;/li&gt;
&lt;li&gt;Generate reports and insights to assist in hospital management and decision-making.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;github-repository&#34;&gt;GitHub Repository&lt;/h3&gt;
&lt;p&gt;Explore the complete code and implementation details on my &lt;a href=&#34;https://github.com/Manodeepray/akamai-care&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;live-demo&#34;&gt;Live Demo&lt;/h3&gt;
&lt;p&gt;Check out the live demo of AkamaiCare &lt;a href=&#34;https://akamai-care.streamlit.app/&#34;&gt;streamlit app&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for checking out AkamaiCare! Feel free to reach out with any feedback or questions.&lt;/p&gt;
</description>
      
      
    </item>
    
    <item>
      <title>Chat Bot for College</title>
      <link>http://localhost:1313/website/projects/chat-bot-for-college/</link>
      <pubDate>Wed, 04 Sep 2024 22:41:27 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/chat-bot-for-college/</guid>
      <description>&lt;h2 id=&#34;chat-bot-for-college&#34;&gt;Chat Bot for College&lt;/h2&gt;
&lt;p&gt;Welcome to my chatbot project for the School of Electronics, KIIT University! This chatbot is designed to make information retrieval easier and more efficient for students and faculty.&lt;/p&gt;
&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Built using a Retrieval-Augmented Generation (RAG) model for intelligent responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Processes data in .csv, .pdf, and .txt formats collected from the official SOEE, KIIT website.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Uses FAISS vector databases indexed with HuggingFace embeddings for efficient retrieval.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Employs an Ensemble Retriever with weighted averaging and a History-Aware Retriever for seamless multi-turn conversations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Supports Gemini 1.5 Flash, HuggingFace models (serverless inference), and Ollama on-device models, providing flexibility in model selection.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interactive and user-friendly interface created with Streamlit for smooth interactions and queries.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;github-repository&#34;&gt;GitHub Repository&lt;/h3&gt;
&lt;p&gt;Check out the code and details of this project on my &lt;a href=&#34;https://github.com/Manodeepray/kiit-chatbot-llm&#34;&gt;GitHub Repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you for exploring this project! Feel free to connect with me if you have suggestions or questions.&lt;/p&gt;
</description>
      
      
    </item>
    
    <item>
      <title>Oralcare_ai</title>
      <link>http://localhost:1313/website/projects/oralcare_ai/</link>
      <pubDate>Wed, 24 Jul 2024 12:32:52 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/oralcare_ai/</guid>
      <description>&lt;h2 id=&#34;its-my-summer-internship-project-at-iit-bhilai&#34;&gt;Its my Summer Internship Project at IIT Bhilai&lt;/h2&gt;
&lt;p&gt;During my internship, I worked on an innovative deep learning project focusing on cancer prediction. I developed a web app with Gradio that assists in diagnosing and evaluating oral diseases. Here‚Äôs a detailed breakdown of the project:&lt;/p&gt;
&lt;h3 id=&#34;part-1-image-classification&#34;&gt;Part 1: Image Classification&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; YOLOv8n&lt;br&gt;
&lt;strong&gt;Task:&lt;/strong&gt; Predicts the type of oral disease (cancer, dental caries, scurvy, periodontal, healthy) using image classification on custom data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced Detection:&lt;/strong&gt; If cancer is detected, the model then uses object detection to pinpoint the growth location and determines the grade and stage:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grades:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Grade 1:&lt;/strong&gt; Similar to normal cells&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grade 2:&lt;/strong&gt; Slightly abnormal&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grade 3:&lt;/strong&gt; Very abnormal&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Stages:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stage 1:&lt;/strong&gt; Less than 2 cm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stage 2:&lt;/strong&gt; 2 cm to 4 cm&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stage 3:&lt;/strong&gt; Greater than 4 cm to widespread growth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Data Collection:&lt;/strong&gt; All images were gathered and processed by me using Google extensions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Workflow:&lt;/strong&gt; The images are fed into the trained YOLOv8n model, which first classifies the disease type. If the disease is classified as cancer, the model performs object detection to identify the specific location of the cancerous growth. The detected area is then analyzed to determine the grade and stage of the cancer based on predefined criteria.&lt;/p&gt;
&lt;h3 id=&#34;part-2-symptom-based-prediction&#34;&gt;Part 2: Symptom-Based Prediction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; Symptoms entered by the patient&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Retrieval-Augmented Generation (RAG) model on custom PDFs&lt;br&gt;
&lt;strong&gt;Functionality:&lt;/strong&gt; Predicts the grade and stage of oral cancer based on patient symptoms, followed by treatment recommendations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grade and Stage Prediction:&lt;/strong&gt; The RAG model takes the patient‚Äôs symptoms and matches them with information from custom PDFs to determine the grade and stage of cancer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Treatment Recommendations:&lt;/strong&gt; Two additional RAG models provide treatment options based on the determined grade and stage, sourcing information from specific PDFs that detail treatments for each grade and stage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM used:&lt;/strong&gt; Mistral 7B&lt;/p&gt;
&lt;h3 id=&#34;challenges-and-limitations&#34;&gt;Challenges and Limitations&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;LLM Constraints:&lt;/strong&gt; Due to size limitations (maximum 10GB on Hugging Face), a lighter LLM was utilized, necessitating the creation of three distinct RAG models (two for treatment recommendations and one for grade and stage evaluation).&lt;br&gt;
&lt;strong&gt;Multiple API Calls:&lt;/strong&gt; Multiple calls to the LLM were required for coherent responses, which could be optimized with a more robust model such as GPT-3 or Gemini. A single, more powerful LLM would enhance processing efficiency and accuracy.&lt;br&gt;
&lt;strong&gt;Performance:&lt;/strong&gt; The current RAG model setup results in longer processing times, impacting the speed of output generation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/posts/manodeep-ray-346b3b294_deeplearning-cancerprediction-ai-activity-7219962641518067712--nht?utm_source=share&amp;amp;utm_medium=member_desktop&#34;&gt;Linked in post&lt;/a&gt;
&lt;a href=&#34;https://github.com/Manodeepray/oral_health_ai&#34;&gt;Github repo &lt;/a&gt;&lt;/p&gt;
</description>
      
      
    </item>
    
    <item>
      <title>Offline_ai</title>
      <link>http://localhost:1313/website/projects/offline_ai/</link>
      <pubDate>Wed, 24 Jul 2024 12:26:09 +0530</pubDate>
      
      <guid>http://localhost:1313/website/projects/offline_ai/</guid>
      <description>&lt;h1 id=&#34;building-an-offline-sms-llm-app-for-buildspace-my-journey-and-challenges&#34;&gt;Building an Offline SMS LLM App for Buildspace: My Journey and Challenges&lt;/h1&gt;
&lt;p&gt;Hello everyone! It&amp;rsquo;s Manodeep here again, back with another update on my exciting adventures in the world of Machine Learning and AI. Today, I want to share my ongoing project for Buildspace and some of the challenges I&amp;rsquo;m facing along the way.&lt;/p&gt;
&lt;h2 id=&#34;the-project-offline-sms-llm-app&#34;&gt;The Project: Offline SMS LLM App&lt;/h2&gt;
&lt;p&gt;Currently, I&amp;rsquo;m working on an offline SMS LLM (Large Language Model) app for Buildspace. The idea is to create an application that leverages the power of language models to provide intelligent responses via SMS, all while working offline. This project is both challenging and exhilarating, pushing the boundaries of what can be achieved with limited connectivity.&lt;/p&gt;
&lt;h2 id=&#34;why-an-offline-sms-llm-app&#34;&gt;Why an Offline SMS LLM App?&lt;/h2&gt;
&lt;p&gt;In many parts of the world, internet access is still a luxury. By developing an offline SMS LLM app, I aim to bring the benefits of AI to areas with limited or no internet connectivity. This app can help with various tasks, from educational assistance to quick information retrieval, all through simple SMS.&lt;/p&gt;
&lt;h2 id=&#34;the-journey-so-far&#34;&gt;The Journey So Far&lt;/h2&gt;
&lt;h3 id=&#34;initial-research-and-planning&#34;&gt;Initial Research and Planning&lt;/h3&gt;
&lt;p&gt;The journey began with extensive research. I needed to understand the limitations and possibilities of running a language model offline and how to integrate it with SMS functionality. This involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Selecting the Right Model:&lt;/strong&gt; Choosing a language model that can operate efficiently offline. Smaller, more efficient models like Mistral 7B came into consideration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Understanding SMS Protocols:&lt;/strong&gt; Diving into the intricacies of SMS protocols and how to interact with them programmatically.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;development-phase&#34;&gt;Development Phase&lt;/h3&gt;
&lt;p&gt;With a solid plan in place, I started the development phase. Here&amp;rsquo;s a snapshot of the process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model Optimization:&lt;/strong&gt; Ensuring the language model can run smoothly on devices with limited resources.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SMS Integration:&lt;/strong&gt; Developing the SMS interface to send and receive messages seamlessly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Interface:&lt;/strong&gt; Creating a simple and intuitive interface for users to interact with the app.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-progress&#34;&gt;The Progress&lt;/h2&gt;
&lt;h1 id=&#34;progess-till-24724&#34;&gt;Progess till 24.7.24&lt;/h1&gt;
&lt;p&gt;Now currently facing an issue of integrating the SMS interface . i have done everything but the number used by me is international number ..and my phone doesnt have international roaming :..so i cant test my app &amp;hellip; so yeaaaa.&lt;/p&gt;
</description>
      
      
    </item>
    
  </channel>
</rss>
