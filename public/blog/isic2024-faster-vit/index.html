<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title>Paper on Faster-Vit Archietecture(not yes published) · Manodeep Ray</title>
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<link rel="canonical" href="https://manodeepray.is-a.dev/website/blog/isic2024-faster-vit/">

		

		<link rel="stylesheet" href="https://manodeepray.is-a.dev/website/node_modules/normalize.css/normalize.css">
		<link rel="stylesheet" href="https://manodeepray.is-a.dev/website/node_modules/firacode/distr/fira_code.css">
		<link rel="stylesheet" href="https://manodeepray.is-a.dev/website/css/nanum-gothic-coding.css">
		<link rel="stylesheet" href="https://manodeepray.is-a.dev/website/css/style.css">
		<script defer data-domain="aakash.is-a.dev" src="https://analytics.is-a.dev/js/script.js"></script>
	</head>
	<body>


<header>
<h1>光</h1>
</header>

<div id="main">
	<nav><a href="https://manodeepray.is-a.dev/">Home</a> &middot;
	<a href="https://manodeepray.is-a.dev/website/blog">Blog</a> &middot;
	<a href="https://manodeepray.is-a.dev/website/projects">Projects</a> &middot;
	<a rel="me" href="https://manodeepray.is-a.dev/website/about">About</a> &middot;
	<a rel="me" href="https://github.com/Manodeepray">GitHub</a> &middot;
	<a rel="me" href="https://www.linkedin.com/in/manodeep-ray-346b3b294/">LinkedIn</a>
</nav>


	<main>


<article >
	<header>
		<h1>Paper on Faster-Vit Archietecture(not yes published)</h1>
		
			<time datetime='2024-09-05T20:33:07&#43;05:30'>2024-09-05</time>
		
	</header>

	<h1 id="exploring-vision-transformers-for-skin-cancer-classification-a-deep-learning-journey">Exploring Vision Transformers for Skin Cancer Classification: A Deep Learning Journey</h1>
<h2 id="introduction">Introduction</h2>
<p>Skin cancer remains one of the most common cancers globally, making early diagnosis and accurate classification essential. In recent years, deep learning has played a pivotal role in automating the detection and classification of skin lesions. Vision Transformers (ViTs) are emerging as a powerful tool for image classification tasks, including medical image analysis.</p>
<p>In this blog, I will share insights from my project, where I applied ViTs to classify skin lesions from the ISIC 2024 dataset into benign and malignant categories. I also explored a novel ViT model incorporating a token learning layer to optimize training time. Let&rsquo;s dive into the details!</p>
<h2 id="dataset-overview-isic-2024">Dataset Overview: ISIC 2024</h2>
<p>The ISIC 2024 dataset is a comprehensive collection of skin lesion images, serving as a benchmark for melanoma detection. The dataset includes various types of skin lesions, making it ideal for training and evaluating deep learning models for binary classification tasks. The key challenge here is to develop AI algorithms that can distinguish histologically confirmed malignant skin lesions from benign ones.</p>
<p>To mimic non-dermoscopic images, this dataset uses standardized cropped lesion-images of lesions from 3D Total Body Photography (TBP). Vectra WB360, a 3D TBP product from Canfield Scientific, captures the complete visible cutaneous surface area in one macro-quality resolution tomographic image. An AI-based software then identifies individual lesions on a given 3D capture. This allows for the image capture and identification of all lesions on a patient, which are exported as individual 15x15 mm field-of-view cropped photos. The dataset contains every lesion from a subset of thousands of patients seen between the years 2015 and 2024 across nine institutions and three continents.</p>
<h2 id="vision-transformers-vits">Vision Transformers (ViTs)</h2>
<p>ViTs represent a paradigm shift in image classification. Unlike traditional methods, which rely on convolutional layers, ViTs use a transformer architecture that originated in natural language processing (NLP). The transformer model processes images as sequences of patches (tokens) and learns the relationships between them.</p>
<p>The main ViT models I tested were:</p>
<ul>
<li>Standard ViT</li>
<li>ViT with a token learning layer</li>
</ul>
<p>The token learning layer aimed to reduce training time by refining the input tokens before they were fed into the transformer model. This optimization was particularly useful for large datasets like ISIC 2024.</p>
<h2 id="results-and-comparison">Results and Comparison</h2>
<p>ViTs demonstrated strong performance in classifying skin lesions. The ability of ViTs to capture long-range dependencies between image patches proved beneficial in medical image analysis, especially with the token learning layer model, which improved both accuracy and efficiency.</p>
<h3 id="key-findings">Key Findings</h3>
<ul>
<li><strong>ViTs:</strong> Outperformed traditional methods in terms of accuracy and training efficiency.</li>
<li><strong>Token Learning:</strong> The token learning layer in ViTs significantly reduced training time while maintaining high accuracy.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>This project highlighted the potential of Vision Transformers in the field of medical image analysis, specifically skin cancer classification. ViTs, with their ability to model global context and dependencies, offer a promising alternative to traditional deep learning methods. As ViTs continue to evolve, they could become the go-to model for various computer vision tasks in healthcare.</p>
<p>In future work, I plan to explore hybrid models that combine the strengths of ViTs with other architectures, as well as investigate the interpretability of these models in clinical settings.</p>
<p>Thanks for reading! Stay tuned for more deep learning adventures!</p>
<hr>
<p><em>Code <a href="https://github.com/Manodeepray/Visual_transformer_paper_code">Github</a></em></p>


	
</article>

	</main>

	<footer>
		© 2025 
		<a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/"><img alt="Creative Commons License" src="https://i.creativecommons.org/l/by/3.0/us/80x15.png"></a>
	</footer>
</div>

	</body>
</html>


