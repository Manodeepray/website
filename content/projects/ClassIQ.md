+++
title = 'ClassIQ'
date = 2025-04-23T03:06:53+05:30
draft = false
+++

Got it! Here's the updated blog-style `.md` with that additional context integrated. I've added a section near the top that sets the scene as a **college minor project**, clarified the **Streamlit demo's purpose**, and explained how the **server-based pipeline** works.

---

```markdown
# Building a Classroom Monitoring System with Face Recognition and AI-Powered Lecture Notes ğŸ§ ğŸ“¸

> ğŸ“š *This was my 3rd-year college minor project,* built to explore how AI can enhance classroom engagement, automate attendance, and help with structured note generation.

---

## ğŸ’¡ The Vision

The **Classroom Monitoring System** is a server-based AI pipeline using **computer vision**, **speech recognition**, and **LLMs** to:
- Detect faces and recognize student identities
- Track attentiveness throughout a session
- Transcribe and summarize lectures into structured markdown notes
- Store and visualize data on a cloud dashboard

While the system is designed to work with **edge devices** (like a classroom Raspberry Pi or local camera unit), I built a **simple Streamlit app** to demo the functionality on a **single video** on your laptop.

---

## ğŸ§© How It Works

The core system is designed as a **server-based pipeline**. Once deployed:

1. An **edge device** records a classroom session (video + audio).
2. The **URL of the processing server (ngrok)** is entered into the edge device.
3. Every time a new video is sent, the server:
   - Detects and recognizes faces
   - Tracks attentiveness
   - Transcribes the audio
   - Uses an LLM to generate structured notes
   - Saves all output to cloud storage

The **Streamlit UI** (`apps/demo_app.py`) showcases this pipeline for a **single demo video**, giving a preview of the end-to-end process.

---

## âœ… Core Features

âœ… **Face Detection and Recognition for Attendance**  
âœ… **Frame-Based Attentiveness Tracking**  
âœ… **Audio Transcription via Vosk**  
âœ… **LLM-Powered Summarization (Groq's `llama3-8b-8192`)**  
âœ… **Streamlit-Based UI for Demos & Data Exploration**  

---

## ğŸ§  Tech Stack

| Layer             | Tools & Libraries                            |
|------------------|----------------------------------------------|
| Vision            | YOLOv11, YuNet (OpenCV)                      |
| Transcription     | [Vosk Small EN](https://alphacephei.com/vosk/models) |
| Summarization     | Groq LLM (`llama3-8b-8192`)                  |
| Backend           | Flask, tmux, ngrok                           |
| UI/Dashboard      | Streamlit                                    |
| Deployment        | Lightning AI                                 |

---

## âš™ï¸ Getting Started

```bash
# Clone the repo
git clone <repo-url>
cd <repo-folder>

# Create a virtual environment
python3 -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Install system packages (Linux)
sudo apt install ffmpeg -y

# (Optional) Install tmux for background processes
sudo apt install tmux

# Add your Groq API key to a .env file
echo 'GROQ_API_KEY="your_key_here"' > .env
```

---

## ğŸš€ Usage Instructions

### ğŸ‘¤ Training Your Own Face Recognition Model

1. Add your dataset: `dataset/{student_name}/{face_images}`
2. Update the dataset path in `src/training/train_face_rec.py`
3. Generate YOLO-ready data:
   ```bash
   python src/training/get_training_data.py
   ```
4. Train the model:
   ```bash
   bash scripts/train.sh
   ```

### ğŸ–¥ï¸ Running the Server Pipeline

```bash
# Optionally use tmux to manage long processes
bash scripts/run_servers.sh

# Or run components manually
python processor.py
python server.py
ngrok http 5000
```

âœ… Once deployed, enter the **ngrok link** in your edge device. Each uploaded classroom session video will be processed and results generated automatically.

### ğŸŒ Try the Demo UI

```bash
# Demo for single video
streamlit run apps/demo_app.py

# View saved notes and data
streamlit run apps/files_ui.py
```

---

## ğŸ› ï¸ Customizing Summarization

You can switch the LLM or update the prompt for note generation in:
```python
# src/pipelines/core/llm_integration.py
GROQ_LLM_MODEL_ID = "llama3-8b-8192"
```

And edit the prompt in `generate_notes()` to match your desired note structure.

---

## ğŸ“Š Results & Observations

- **Face Recognition Accuracy**: 91.67% (Top-1)
- **Attentiveness Metric**:
  ```
  Attentiveness = (frames student is visible) / (total frames)
  ```
- **Lecture Notes Quality**: Depends on LLM + prompt, human-evaluated

---

## ğŸ”® Future Plans

- ğŸ§  Shift to custom facial-points-based recognition model
- ğŸ“¢ Add real-time inattentiveness alerts
- ğŸ§ª Refine LLM prompts for more structured results
- â˜ï¸ Switch from CSV to PostgreSQL for scalable dashboard
---

## ğŸ™Œ About Me

**Manodeep Ray** â€“ Passionate about deep learning, LLM , CV, and building real-world systems. This project was built as part of my **college 3rd-year minor**, blending CV + NLP in an educational setting.

---

## ğŸ“„ License

Licensed under **Creative Commons (CC)**.

---

Thanks for checking it out! If you liked this, drop a â­ on [GitHub](#) or connect with me to chat about AI + education ğŸš€
```

---

Let me know if you want a matching social media caption, GitHub repo description, or thumbnail image to go with this blog!
